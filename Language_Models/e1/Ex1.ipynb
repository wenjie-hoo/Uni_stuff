{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"> Language Models Exercise 1</h3>\n",
    "<h5 style=\"text-align: center;\"> Wenjie Hu 343312</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Zadanie 1$. Come up with a type of task that is relatively easy for humans (not relying on factual knowledge), with a clearly defined answer 1 that ChatGPT struggles with (alternatively: show three non-trivial tasks that Chat can do, which somewhat surprised you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User: can you count how much i over there iiiiiiiiiiiiiiiiiiii   \n",
    "GPT-3: There are 18 \"i\"s in \"iiiiiiiiiiiiiiiiiiiii\"!   \n",
    "   \n",
    "There are acually 20 \"i\"s in \"iiiiiiiiiiiiiiiiiiiii\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e2 \n",
    "Choose 10 rather challenging questions from Task 4 in Workshop 1, and then check how many of them are correctly solved by ChatGPT. You can also decide to use other trivia questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q：What moves through cities without feet, passing on a flame that never depletes?   \n",
    "Answer: The Olympic Torch.  \n",
    "GPT: light    \n",
    "\n",
    "Q: Five colorful rings interlock without strain, a symbol of nations together in the same game.  \n",
    "Answer: The Olympic Rings.  \n",
    "GPT: Olympic Rings  \n",
    "  \n",
    "Q: What follows you day and night, yet flees when the light shines bright?  \n",
    "Answer: shadow.  \n",
    "GPT: shadow  \n",
    "  \n",
    "Q: What speaks without a mouth and answers with no voice, yet its sound always brings back your choice?  \n",
    "Answer: Echo.  \n",
    "GPT: Echo  \n",
    "  \n",
    "Q: I connect the lost and found, though I am neither seen nor bound, I weave through life, holding all around.  \n",
    "Answer: A memory.  \n",
    "GPT: love  \n",
    "  \n",
    "Q: I come without warning and leave without trace, I carry both joy and sorrow in my embrace.  \n",
    "Answer: A dream.  \n",
    "GPT: time  \n",
    "\n",
    "Q:  I have no beginning and no end, I am always moving, though I never bend.  \n",
    "Answer: A river.  \n",
    "GPT: circle  \n",
    "  \n",
    "Q: I brighten your path though you cannot see me, I travel in silence, yet I can never be free.  \n",
    "Answer: The moon.  \n",
    "GPT: star  \n",
    "  \n",
    "Q: I am as old as time but have no age, I roam the earth but leave no trace, I am sought after but cannot be caught.  \n",
    "Answer: The wind.  \n",
    "GPT: wisdom  \n",
    "  \n",
    "Q: I hang around necks, but I’m no necklace; a mark of victory and strenuous practice.  \n",
    "Answer: A gold medal  \n",
    "GPT:  medal  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e3\n",
    "In the case of language models, a watermark is a property assigned to generated texts that is invisible without specialized watermark detection methods. A certain language model provider proposed the following solution: add a condition (without notifying anyone) to prefer words starting with the letters V, S, and K during generation. Expand on this idea by considering how (and if) such a watermark would work for long texts. And for short texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html   \n",
    "The frequency of the letter V, S, and K is 1.5%, 11%, and 1% respectively.   \n",
    "In long texts, words starting with V, S, and K can be included frequently without affecting readability, creating a clear watermark by indentifying the frequency of the letter V and K, because they have a less frequency in English, large number of words makes the words starting with V and K \"disappear\".  \n",
    "However, in short texts, the watermark may not be so obvious, limited word count restricts the inclusion of V, S, and K words, risking coherence and readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e5\n",
    "Describe a procedure that, using a basic language model text generation interface, solves the problem of generating exactly one word (for a given prefix consisting of whole words) as\n",
    "efficiently as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Step 1: Define prefix and prepare prompt  \n",
    "    # Step 2: Configure the max_tokens = 10(typical length of words)\n",
    "    # Step 3: Generate text from the prompt  \n",
    "    # Step 4: Extract the first word from the generated text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next word is: iced\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def language_model_generate(prompt, max_tokens):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=len(input_ids[0]) + max_tokens, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def generate_next_word(prefix):  \n",
    "    # define prefix and prompt  \n",
    "    prompt = prefix + \" \"  \n",
    "    # max tokens to generate      \n",
    "    max_tokens = 10 \n",
    "    generated_text = language_model_generate(prompt, max_tokens)  \n",
    "    words = split_into_words(generated_text)  \n",
    "      \n",
    "    if len(words) > len(split_into_words(prefix)):  \n",
    "        next_word = words[len(split_into_words(prefix))]  \n",
    "        return next_word  \n",
    "    else:  \n",
    "        return None\n",
    "  \n",
    "def split_into_words(text):  \n",
    "    return text.split()  \n",
    "  \n",
    "prefix = \"I need a bottle of\"\n",
    "next_word = generate_next_word(prefix)  \n",
    "print(\"The next word is:\", next_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e6\n",
    " Read and discuss the biases of the papuGaPT model. How were these biases studied? What conclusions can be drawn from these studies? (section Bias Analysis on the page https://huggingface.co/flax-community/papuGaPT2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/flax-community/papuGaPT2/blob/main/papuGaPT2_bias_analysis.ipynb  \n",
    "The author took the sentence - \"She works as\" as a prompt and asked the GPT to complete it 50 times. Then they took the beginnings of the generated answers and placed them in a string and  ade word clouds, showing the most popular ones. \n",
    "\n",
    "The sencond, 1,000 texts were generated to assess ethnic, national, and gender prejudice using prompts like: Person: [German, Gypsy, Jew, Ukrainian, He/She]. Each combination of 5 nationalities, 2 genders, and 5 actions yielded 20 texts: 5 * 2 * 5 * 20 = 1,000.\n",
    "\n",
    "A table was created with prompt parameters, generated text, and corresponding nationality. This data was then processed by a model for detecting hate speech in Polish. To avoid errors, sentence beginnings were removed before inputting into the model. The results were aggregated and averaged, then presented in a table.\n",
    "\n",
    "The couclusion is that the model is biased, they do not recommend using the GPT2 model outside of research unless bias can be clearly mitigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e8\n",
    " Assume we have two language models and want to generate text using knowledge from both. Propose 3 different scenarios, with at least one that does not assume the same tokenization in both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complementation of Models  \n",
    "Model A is designed for generating structured content and excels in creating outlines or skeletons for longer texts. Meanwhile, Model B specializes in providing detailed and expert information on specific topics. The output from Model A serves as the framework for Model B to elaborate on.  \n",
    "\n",
    "- Feedback Loop  \n",
    "Model A generates initial text in any language, and Model B provides feedback, which Model A then incorporates into subsequent generations.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
