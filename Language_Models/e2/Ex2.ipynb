{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"> Language Models Exercise 2</h3>\n",
    "<h5 style=\"text-align: center;\"> Wenjie Hu 343312</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e1\n",
    "- General formula for Perplexity  \n",
    "$P=(\\prod_{i=1}^{N}P(w_i))^{-\\frac{1}{N}}$  \n",
    "\n",
    "- Unigram Model  \n",
    " Each block has the same probability \n",
    "$P(w) = \\frac{1}{10}$   \n",
    "$P = \\left(\\left(\\frac{1}{10}\\right)^N\\right)^{-\\frac{1}{N}} = \\left(\\frac{1}{10}\\right)^{-1} = 10$\n",
    "\n",
    "- Bigram Model\n",
    "The probability of one block following another is uniform\n",
    "$P(w_{i+1}|w_i = \\frac{1}{10})$   \n",
    "$P = \\left(\\left(\\frac{1}{10}\\right)^N\\right)^{-\\frac{1}{N}} = \\left(\\frac{1}{10}\\right)^{-1} = 10$\n",
    "\n",
    "- n-gram Model\n",
    "blocks are completely independent  \n",
    "$n = 1$ \n",
    "$P = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e2 \n",
    "most of LLM are trained to maximize the probability of the next word given a context, which basically lowers the perplexity of the generated text. For high perplexity, the text may have lots of unexpected expressions and words. but for low perplexity, the text is more likely to be fluent and short sentences.  \n",
    "  \n",
    "- why this is not an idea solution?   \n",
    "\n",
    "LLM  has high variability, different model may have different perplexity setting. e.g. creative writing may have high perplexity. on anther hand, well-written human texts with clear, concise language might also have low perplexity. \n",
    "\n",
    "- how can a user of a model like ChatGPT control the perplexity of generated text?  \n",
    "\n",
    "we can use some specific prompt to control the perplexity of generated text. e.g. if we want to generate a text with high perplexity, we can use some creative writing prompt. if we want to generate a text with low perplexity, We can control perplexity by prompt like (\"make a text dull and repetitive\") and by setting the temperature to a low value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e3\n",
    "a)   \n",
    "- Start with the predefined word at position k.\n",
    "- going right (for M−k words) using n-gram probabilities conditioned\n",
    "- going left (for k-1 words) using thesame probabilities \n",
    "- Combine them to get the final text\n",
    "  \n",
    "b)\n",
    "  \n",
    "\n",
    "c)  \n",
    "from the first word to right, there is no guarantee that the last word will with a reasonable probability, the last workd might have a low probability.\n",
    "- generate from both directions\n",
    "- when the two generated text meet at a certain position k, choose the wrod that matches in both directions with a higheset probability,  then we can combine them to get the final text.  \n",
    "\n",
    "d) meet the condition of might have a low probability, and take a long time to generate the text.\n",
    "- use a vocabulary filter to identify words with the required suffix\n",
    "- for position $i$, sample the only form  predefined word that match $S_i$\n",
    "- Use 2-3 gram probabilities conditione on the previous words\n",
    "- If no valid word exists for a position, backtrack to the previous step and re-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e4  \n",
    "protenial applications: Palindrome detection or generation. funny wrod game.  \n",
    "maybe hopefull for the research of language that read from right to left(araic, hebrew, etc) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e5\n",
    "-  use LLM to generate texts, first letter of each sentence to form part of a secret message. \n",
    "The overall letter would read normally, but the first letters of each sentence would spell out additional information. \n",
    "(high risk of being detected)\n",
    "-  Use the language model to generate text where specific words or synonyms are chosen to represent bits of information. For example, every fifth word could be a specific choice from the message, and the order of the words could represent a message.\n",
    "- To covertly send a message, convert each letter to its ASCII code, and guide a language model to generate text where specific tokens correspond to these ASCII values. The message is hidden in the sequence of tokens and can be decoded by knowing the encoding method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e6\n",
    "\n",
    "It states that a cryptosystem should remain secure even if an adversary knows all the details of the system, except for the secret decryption key.\n",
    "\n",
    "The first and second ideas don't meet the requirement of the Kerckhoff principle because there is no secret key.\n",
    " \n",
    "the third one met the principle.the security of the encoded message could rely on the key used to encrypt the ASCII values before embedding them into the text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e8\n",
    "The phenomenon arises because tokens that start meaningful words typically begin with a space, whereas tokens that complete words do not have a space in front of them. Therefore, if a space is added at the end of the prompt, the highest probability tokens will likely be those without a leading space, i.e., tokens that complete words. Without a trailing space, the highest probability tokens will likely be those that start words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all other warnings\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'  # Suppress transformer warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without space: kwiat, który jest symbolem miłości i pokoju.\n",
      "W tym roku, w ramach obchodów 100-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With space: kwiat owy, który w tym roku obchodził swoje święto.\n",
      "W tym roku, w\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without space: kawalerka w centrum miasta, w pobliżu sklepy, restauracje, kawiarnie, park, basen\n",
      "With space: kawa ..........\n",
      "W tym roku w ramach akcji \"Zima w mieście\" w naszej\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"flax-community/papuGaPT2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=20, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Without space:\", generate_text(\"kwiat\"))\n",
    "print(\"With space:\", generate_text(\"kwiat \"))\n",
    "print(\"-\" * 40)\n",
    "print(\"Without space:\", generate_text(\"kawa\"))\n",
    "print(\"With space:\", generate_text(\"kawa \"))\n",
    "print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the result\n",
    "\n",
    "Without space: kwiat, który jest symbolem miłości i pokoju.  \n",
    "With space: kwiat owy, który w tym roku obchodził swoje święto.  \n",
    "  \n",
    "Without space: kawalerka w centrum miasta, w pobliżu sklepy, restauracje, kawiarnie, park, basen  \n",
    "With space: kawa .......... W tym roku w ramach akcji \"Zima w mieście\" w naszej\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e9\n",
    "Specific rhymes at the end of lines, despite rhyme and rhythm, and the text make sense overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem Prompt Tokens:\n",
      "Token 0: \n",
      "\n",
      "Token 1: I\n",
      "Token 2:  have\n",
      "Token 3:  only\n",
      "Token 4:  one\n",
      "Token 5:  burning\n",
      "Token 6:  desire\n",
      "Token 7: \n",
      "\n",
      "\n",
      "Top K Tokens from Last Token in Prompt:\n",
      "top_k 1: \n",
      "\n",
      "top_k 2: I\n",
      "top_k 3: (\n",
      "top_k 4: .\n",
      "top_k 5: The\n",
      "top_k 6:  (\n",
      "top_k 7: A\n",
      "top_k 8: [\n",
      "top_k 9: '\n",
      "top_k 10: \"\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "I have only one burning desire\n",
      "\n",
      "As I get older, my anger goes deeper...\n",
      "\n",
      "When I get older, I'll need the strength I need to try harder.\n",
      "\n",
      "And even with those strong feelings, this isn't worth it.\n",
      "\n",
      "It's hard to\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, set_seed\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelWithLMHead.from_pretrained(MODEL_NAME)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "I have only one burning desire\n",
    "\"\"\"\n",
    "\n",
    "def print_tokens(input_ids):\n",
    "    print(\"Poem Prompt Tokens:\")\n",
    "    poem_tokens = [tokenizer.decode(token_id) for token_id in input_ids[0]]\n",
    "    for i, token in enumerate(poem_tokens):\n",
    "        print(f\"Token {i}: {token}\")\n",
    "\n",
    "def analyze_logits(logits):\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    top_k_probs = torch.topk(last_token_logits, k=10)\n",
    "    top_k_ids = top_k_probs.indices\n",
    "    top_k_values = top_k_probs.values\n",
    "\n",
    "    print(\"\\nTop K Tokens from Last Token in Prompt:\")\n",
    "    for i, (token_id, prob) in enumerate(zip(top_k_ids, top_k_values)):\n",
    "        top_k_token = tokenizer.decode([token_id])\n",
    "        print(f\"top_k {i + 1}: {top_k_token}\")\n",
    "\n",
    "def generate_text(input_ids):\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=50,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )[0]\n",
    "    return tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "input_ids = tokenizer.encode(PROMPT, return_tensors='pt')\n",
    "print_tokens(input_ids)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits\n",
    "analyze_logits(logits)\n",
    "\n",
    "generated_text = generate_text(input_ids)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
