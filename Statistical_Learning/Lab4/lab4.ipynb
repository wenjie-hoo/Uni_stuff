{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"> Statistical Learning Lab4</h3>\n",
    "<h5 style=\"text-align: center;\"> Wenjie Hu 343312</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "**Assume the linear model:**\n",
    "$$ Y = X \\beta + \\epsilon$$ \n",
    "where $X'X=I$ and $\\epsilon \\sim N(0,\\sigma^2I)$  \n",
    "**Find the numerical solution for the elastic net in the form:**\n",
    "$$\\hat{\\beta_{en}} = argmin_b \\frac{1}{2} \\|Y-Xb\\|_2^2 + \\lambda\\biggl(\\frac{1}{2}(1-\\alpha)\\|b\\|_2^2 + \\alpha\\sum\\limits_{i=1}^p|b_i| \\biggr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What would be the value of the elastic net estimator with $\\lambda = 1$ and $\\alpha = 0.5$ if  $\\hat{\\beta}_{OLS} = 3?$**\n",
    "- **How does the number of discoveries depend on the parameter $\\alpha$**\n",
    "- **Provide the numerical value for the expected number of false discoveries when $n = p = 1000$, $p_0 = 950$, $\\sigma = 1$, and $\\lambda = 2$ , and the power of detection of $X_1$ when $\\beta_1 = 3$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $X'X=I$, the OLS solution $\\hat{\\beta}_{OLS}$ is just $X'Y$, then we solve for $\\hat{\\beta}_{EN}$ when $\\hat{\\beta}_{OLS} = 3, \\lambda = 1, \\alpha = 0.5$. We expand the function:\n",
    "$$\n",
    "\\hat{\\beta_{en}} \n",
    "= \\frac{1}{2} \\|Y-Xb\\|_2^2 + \\biggl(\\frac{1}{2}(1-0.5)\\|b\\|_2^2 + 0.5\\sum\\limits_{i=1}^p|b_i| \\biggr) \\\\\n",
    "=\\frac{1}{2} \\|Y-Xb\\|_2^2 + 0.25\\|b\\|_2^2 + 0.5|b|\n",
    "$$\n",
    "Then we evaluate the gradient:  \n",
    "$$\n",
    "   \\nabla_b \\left(\\frac{1}{2} \\|Y - Xb\\|^2_2\\right) = -X'Y + X'Xb = -\\hat{\\beta}_{OLS} + b = -3 + b \\\\\n",
    "\n",
    "   \\nabla_b \\left(0.25 b^2 + 0.5 |b|\\right) = 0.5b + 0.5 \\text{sign}(b)\n",
    "$$\n",
    "setting the gradient to zero for minimization:\n",
    "$$\n",
    "   -3 + b + 0.5b + 0.5 \\text{sign}(b) = 0\n",
    "   1.5b + 0.5 \\text{sign}(b) = 3\n",
    "$$\n",
    "then we solve b for b>0:\n",
    "$$\n",
    "1.5b = \\frac{3-0.5sign(b)}{1.5}=\\frac{2.5}{1.5}\\approx 1.67\n",
    "$$\n",
    "thus $1.67$ or $\\frac{5}{3}$ would be the value of the elastic net estimator with $\\lambda=1,\\alpha=0.5\\text{if} \\hat{\\beta}_{OLS}=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elastic net regularization term is defind as: \n",
    "$\\lambda\\biggl(\\frac{1}{2}(1-\\alpha)\\|b\\|_2^2 + \\alpha\\sum\\limits_{i=1}^p|b_i| \\biggr)$, $\\lambda$ is a constant and $\\alpha$ controls the blend between L1 and L2 regularization, when $\\alpha$ increases, the elastic net behaves more like a lasso model, which tends to select variables by setting many coefficients to exactly zero, Lower values of $\\alpha$ make the model behave more like ridge regression, where coefficients are shrunk towards 0, this leads to fewer coefficients being exactly zero, potentially increasing the number of discoveries, but these discoveries might include more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a significance threshold corresponding to a typical z-value for a 5% two-sided test $t \\approx 1.96$, the probability of exceeding t by error under the null hypothesis $P = 2(1 - \\Phi(t))$, for $t = 1.96, P \\approx 0.05$, so the expected number of false discoveries calculated by:\n",
    "$p_0 \\times P = 950 \\times 0.05 = 47.5$\n",
    "The power for $\\beta_1 = 3$ is very high, by simulation calculation, it almost has a 100% probability of correctly detecting the non-zero effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    " **Why does the LASSO, SLOPE, and elastic net perform variable selection, while ridge regression does not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO:  \n",
    "$$\n",
    "\\hat{\\beta}^{\\text{lasso}} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2} \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}\n",
    "$$\n",
    "SLOPE:\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{b \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2} \\| y - Xb \\|_{\\ell_2}^2 + \\sum_{i=1}^{p} \\lambda_i |b|_{(i)} \\right\\}\n",
    "$$\n",
    "Elastic Net:  \n",
    "$$\n",
    "\\beta \\left\\| y - X\\beta \\right\\|_2^2 + \\lambda \\left( \\alpha \\|\\beta\\|_1 + (1 - \\alpha) \\|\\beta\\|_2^2 \\right)\n",
    "$$\n",
    "Ridge:\n",
    "$$\n",
    "\\hat{\\beta}^{\\text{ridge}} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2} \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\frac{1}{2} \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LASSO, the L1 regularization term $ \\lambda \\sum_{j=1}^{p} |\\beta_j| $ encourages sparsity in the coefficients $\\beta$, the similar for SLOPE, the panalty term $\\sum_{i=1}^{p} \\lambda_i |b|_{(i)} $ also encourages sparsity, this lead to some coefficients beding exactly 0, thus performing variable selection. The Elastic net combines L1 and L2 regularization terms, which allows for both variable selection and multicollinearity. Ridge regression uses L2 regularization termn $\\lambda \\sum_{j=1}^{p} \\beta_j^2$, this shrinks the coefficients but doesn't set any of the term to 0, therefore, ridge regression retains all variables in the model, does not perform variable selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "**Formulate the identifiability condition for LASSO. What does it guarantee in terms of model selection? How does it compare to the irrepresentability condition?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identifiability condition states that a vector $ \\beta $ in $ \\mathbb{R}^p $ is identifiable with respect to the L1 norm and the design matrix $ X $ if for any vector $ \\gamma $ in $ \\mathbb{R}^p $ where $ X\\gamma = X\\beta $ and $ \\gamma \\neq \\beta $, it holds that $ \\|\\gamma\\|_1 > \\|\\beta\\|_1 $.\n",
    "\n",
    "This condition implies that if two different coefficient vectors $ \\beta $ and $ \\gamma $ produce the same outcomes through the design matrix $ X $, the one with the smaller L1 norm is the one associated with the true model.\n",
    "\n",
    "The identifiability condition guarantees that the true coefficient vector \n",
    "$\\beta$ is the sparsest solution among all solutions that could produce the observed outcome, it ensure that LASSO selects the model with the smallest number of non-zero coefficients, thereby promoting sparsity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "**Define SLOPE. How is it different from LASSO in terms of formulations and properties?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SLOPE estimator is defined as the solution to the following optimization problem:\n",
    "$$\n",
    "\\hat{\\beta}^{\\text{SLOPE}} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2} \\left\\| y - X \\beta \\right\\|^2_2 + \\sum_{i=1}^{p} \\lambda_i |\\beta|_{(i)} \\right\\}\n",
    "$$\n",
    "where $y$ is the response vector, $X$ is the design matrix, $\\beta$ is the coefficient vector, $∣\\beta∣$ denotes the i-th largest absolute value of the coefficients in $\\beta$, $\\lambda_i$ are the sorted parameters.  \n",
    "For LASSO estimator is definded as:\n",
    "$$\n",
    "\\hat{\\beta}^{\\text{LASSO}} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2} \\left\\| y - X \\beta \\right\\|^2_2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}\n",
    "$$\n",
    "where the $\\lambda$ is a single non-negative parameter.  \n",
    "As we can see the SLOPE applies a sequence of penalties $\\lambda_i$ to the sorted absolute values of coefficients, allowing for differential shrinkage based on the magnitude of the coefficients, but for LASSO, it uses a single penalty $\\lambda$ for all coefficients, which leads to a sparse solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "**What are knockoffs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knockoffs is a variable selection method that can control the false discovery rate in statistical linear models when there are enough observations. It can be used in conjunction with the Lasso statistic to greatly improve the power of controlling for a high proportion of null variables. a brief overview:  \n",
    "Let X be our design matrix and a copy $\\hat{X}$, we create fake versions of each variable $\\hat{X}_j$ that mimic the original variables' $X_j$ correlations but are independent of the outcome.   \n",
    "Then measure variable importance:  \n",
    "$$W_j = |stat{X_j}| - |stat{\\hat{X}_j}|$$\n",
    "where $X_j$ is the original varible and $\\hat{X}_j$ is the knockoff and *stat* indicates theire inportance.    \n",
    "The final procedure for knockoff is selecting variables, the knockoff selects the varibles with large and positive values of $W_j$, and then a threshold $T$ is chosen to control the FDR. Only variables with $W_j$ ​ exceeding $T$ are retained:\n",
    "$$\n",
    "T = \\min \\left\\{ t: \\frac{ 1+ \\# \\{ j: W_j \\leq -t\\}}{\\#\\{ j: W_j \\geq t\\}} \\leq \\alpha \\right\\} \n",
    "$$\n",
    "where $\\alpha$ is the desired target FDR level.\n",
    "ref: [The Methodology of Knockoffs - Outline](https://web.stanford.edu/group/candes/knockoffs/outline.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "The vector of W statistics for the knockoffs procedure is equal to:\n",
    "W = (8,−4,−2,2,−1.2,−0.6,10,12,1,5,6,7). Which variables would be considered important if we use knockoffs at the false discovery rate (FDR) level q = 0.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a W statistics vector $W = (8,−4,−2,2,−1.2,−0.6,10,12,1,5,6,7)$, we find the random threshold $\\hat{t}\\lambda$ such that $\\hat{t}(\\lambda) = \\min \\{ t > 0 : \\frac{1 + \\#\\{j : W_j(\\lambda) \\leq -t\\}}{\\#\\{j : W_j(\\lambda) \\geq t\\}} \\leq q \\}$, then we sort $W$ in descending order and apply for the threshold formula to computer the ratio for each t and find the minimum $t$ such that the condtion holds. \n",
    "Then We found that the minimum t where the condition holds is $t = 5$, and then we select $\\widehat{S(\\lambda)} =\\{ j:W_j(\\lambda) \\geq \\hat{t}(\\lambda)\\}$ therefore the variables considered important are $\\widehat{S(\\lambda)} = \\{1, 7, 8, 10, 11, 12\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1</li><li>7</li><li>8</li><li>10</li><li>11</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 7\n",
       "\\item 8\n",
       "\\item 10\n",
       "\\item 11\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 7\n",
       "3. 8\n",
       "4. 10\n",
       "5. 11\n",
       "6. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  1  7  8 10 11 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]  8 10 12  5  6  7\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "Show that ridge regression can be viewed as the Maximum A Posteriori\n",
    "(MAP) Bayes rule with a multivariate normal prior on regression coeffi\n",
    "cients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge regression minimizes the following function\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{ridge}} = \\arg \\min_{\\beta} \\left\\{ \\frac{1}{2} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^p \\beta_j^2 \\right\\}\n",
    "$$\n",
    "where $\\beta = (\\beta_0, \\beta_1, ...,\\beta_p)$ are the coefficients, $x_{ij}$ are the predictors, $y_i$ are the respones and $\\lambda$ is the parameter.  \n",
    "A Probabilistic interpretation of the regularization term can be written as:\n",
    "$$\n",
    "\\frac{\\lambda}{2} \\sum_{j=1}^p \\beta_j^2 = \\frac{\\lambda}{2} \\beta^T \\beta\n",
    "$$\n",
    "The ridge penelty can be interpreted as incorporating a prior about the distribution of the coefficients:\n",
    "$$\n",
    "\\beta \\sim N(0, \\frac{1}{\\lambda}I)\n",
    "$$\n",
    "The posterior distribution of $\\beta$ given the data $y$ combines the likelihood and the prior: \n",
    "$$\n",
    "\\pi(\\beta | y, X) \\propto \\mathcal{N}(y | X, \\beta) \\times \\pi(\\beta)\n",
    "$$\n",
    "where: $/mathcal{N}(y | X, \\beta)$ L(y∣X,β) is the likelihood, and $\\pi(\\beta)$ is the prior distribution.  \n",
    "We substitut the forms for the likelihood and the prior, we get:\n",
    "$$\n",
    "\\pi(\\beta | y, X) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} (y - X\\beta)^T (y - X\\beta) \\right) \\times \\exp\\left( -\\frac{\\lambda}{2} \\beta^T \\beta \\right)\n",
    "$$\n",
    "Last, The MAP estimate $\\hat{\\beta}_{MAP}$ maximizes the posterior distribution, or, equivalently, minimizing the negative of the log-posterior:\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MAP}} = \\arg \\min_{\\beta} \\left\\{ \\frac{1}{2\\sigma^2} (y - X\\beta)^T (y - X\\beta) + \\frac{\\lambda}{2} \\beta^T \\beta \\right\\}\n",
    "$$\n",
    "When $\\sigma^2 = 1$, this is equivalent to the ridge regression.\n",
    "Thus, ridge regression be indeed viewed as the Maximum A Posteriori\n",
    "(MAP) Bayes rule with a multivariate normal prior on regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the design matrix $X_{500×450}$ such that its elements are independent and identically distributed (iid) random variables from $N(0,\\sigma = \\sqrt{\\frac{1}{n}}$. Then generate the vector of the response variable according to the model:\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "where $\\epsilon ∼ 2N(0,I), \\beta_i = 10 \\text{ for } i \\epsilon {1,...,k}, \\beta_i = 0 \\text{ for } i \\in {k + 1,...,450}, \\text{ and } k ∈{5,20,50}$.\n",
    "Then we repeat the experiment 100 times, estimate the regression coef-\n",
    "ficients and/or identify important variables using Least squares, Ridge regression and  LASSO with the tuning parameters selected by cross-validation, as well as Knockoffs with ridge and LASSO at the nominal FDR equal to 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are presented below:  \n",
    "<center>\n",
    "\n",
    "**FDR and Power**\n",
    "\n",
    "|   | FDR_Lasso   | Power_Lasso    | FDR_K_Lasso | Power_K_Lasso | FDR_K_Ridge | Power_K_Ridge |\n",
    "|---|-------------|--------------|--------------------|--------------------|--------------------|--------------------|\n",
    "| 5 | 0.1316304   | 0.9340000    | 0.1390819          | 0.8140000          | 0.7463604          | 0.9860000          |\n",
    "| 20| 0.3517620   | 0.9965000    | 0.1702814          | 0.9790000          | 0.3647529          | 0.9420000          |\n",
    "| 50| 0.4455709   | 0.9992000    | 0.1721024          | 0.9892000          | 0.1441629          | 0.8616000          |\n",
    "\n",
    "</center>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "**MSE and MSE_mean Data**\n",
    "|   | MSE_OLS     | MSE_Ridge    | MSE_Lasso    | MSE_mean_OLS | MSE_mean_Ridge | MSE_mean_Lasso |\n",
    "|---|-------------|--------------|--------------|--------------|----------------|----------------|\n",
    "| 5 | 1812.7947257| 499.6594673  | 234.3692781  | 1812.7947257 | 499.6594673    | 234.3692781    |\n",
    "| 20| 1805.0344549| 1821.5322840 | 492.5987870  | 1805.0344549 | 1821.5322840   | 492.5987870    |\n",
    "| 50| 1801.0842219| 2212.2268248 | 811.8213380  | 1801.0842219 | 2212.2268248   | 811.8213380    |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we can observe that Ridge and LASSO generally perform better (lower MSE) than OLS, likely due to their regularization properties which prevent overfitting in the context of many predictors (p > n scenario).  \n",
    "When we look at the FDR and power, Lasso has a FDR of up to 0.4456 and power reaching 0.9992, the Knockoff Lasso and Ridge control FDR more effectively, maintaining it below 0.1721 with the power up to 0.9892. This indicates that knockoff methods provide a better balance between identifying true positives and controlling false discoveries in this scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
