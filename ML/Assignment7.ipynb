{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WQfyik4uG0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marekpiotradamczyk/ml_uwr_22/blob/main/Assignments/Assignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JGmPStV4yiw"
      },
      "source": [
        "# Lab Assignment 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUenJ9L141My"
      },
      "source": [
        "**Submission deadline:**\n",
        "* **lab session in the week 5-9.12.22**\n",
        "\n",
        "**Points: 1+2 points**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsnbuW1uzVcC"
      },
      "outputs": [],
      "source": [
        "# Please note that this code needs only to be run in a fresh runtime.\n",
        "# However, it can be rerun afterwards too.\n",
        "!pip install -q gdown httpimport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4TIgG0bwlpS"
      },
      "outputs": [],
      "source": [
        "# Standard IPython notebook imports\n",
        "import itertools\n",
        "import io\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.optimize as sopt\n",
        "import scipy.stats as sstats\n",
        "import seaborn as sns\n",
        "import sklearn.ensemble\n",
        "import sklearn.tree\n",
        "from sklearn import datasets\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib import animation, pyplot, rc\n",
        "\n",
        "import httpimport\n",
        "\n",
        "# In this way we can import functions straight from github\n",
        "with httpimport.github_repo(\n",
        "    \"janchorowski\", \"nn_assignments\", module=\"common\", branch=\"nn18\"\n",
        "):\n",
        "    from common.gradients import check_gradient\n",
        "    from common.plotting import plot_mat\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts2DMcez4uG9"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "R684Og494uG-",
        "outputId": "75fb5da9-7922-485c-e515-197841580cbf"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "petal_length = iris.data[:, iris.feature_names.index(\"petal length (cm)\")].reshape(-1, 1)\n",
        "petal_width = iris.data[:, iris.feature_names.index(\"petal width (cm)\")].reshape(-1, 1)\n",
        "\n",
        "#get rid of setosa targets\n",
        "petal_length = petal_length[iris.target != 0, :]\n",
        "petal_width  = petal_width[iris.target != 0, :]\n",
        "iris_type    = iris.target[iris.target != 0] - 1\n",
        "\n",
        "plt.scatter(petal_length, petal_width, c=iris.target[iris.target != 0], cmap=\"spring\")\n",
        "plt.xlabel(\"petal_length\")\n",
        "plt.ylabel(\"petal_width\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RWR96lp4uG-"
      },
      "source": [
        "### Logistic Regressoin intuitions\n",
        "\n",
        "Looking at the Iris scaterplot, we intuitively see that we could draw a line which approximately separates the Versicolors from Virginicas. While it will not correctly classify all flowers near the boundary region, it will do a decent job for the more distant ones.\n",
        "\n",
        "One such guesstimated line may be $\\text{petal_length} + \\text{petal_width} - 6.5=0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "GFYXq3JU4uG_",
        "outputId": "b941f38c-76f0-4c60-9d56-9c8b61c4b2c8"
      },
      "outputs": [],
      "source": [
        "# Extract the petal_length and petal_width of versicolors and virginicas\n",
        "IrisX = np.hstack([np.ones_like(petal_length), petal_length, petal_width])\n",
        "\n",
        "# Set versicolor=0 and virginia=1\n",
        "IrisY = iris_type.reshape(-1, 1).astype(np.float64)\n",
        "\n",
        "plt.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY, cmap=\"spring\")\n",
        "plt.xlabel(\"petal_length\")\n",
        "plt.ylabel(\"petal_width\")\n",
        "ylim = (0.0, 3.)\n",
        "print(ylim)\n",
        "\n",
        "# Guesstimate a separation boundary\n",
        "plt.plot(plt.xlim(), 6.5 - np.array(plt.xlim()))\n",
        "plt.ylim(*ylim)\n",
        "_ = plt.text(3,2,\"PL + PW - 6.5 = 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pit42MH04uHA"
      },
      "source": [
        "### Regression with a squashing function\n",
        "\n",
        "Getting back to classification problem, we want to model the conditional probability \n",
        "$$\n",
        "p(\\text{class}|\\text{petal length}, \\text{petal width}) = p(y|x)\n",
        "$$\n",
        "\n",
        "This conditional probability should depend on the distance of a point to the separating boundary. Points near the separating line are ambiguous - probablity there should be close to 0.5. On the other hand, the model should be fairly certain on points far away from the separating boundary.\n",
        "\n",
        "We thus need to squash the distance from the boundary to the $0-1$ range of valid probabilities. We will accomplish this by mapping the distance through a \"sigmoid\" (meaning S-shaped function). A very popular function is the logistic sigmoid:\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "HDgfq6ht4uHA",
        "outputId": "8f0bbd91-4a76-4db0-de56-43ac940034b3"
      },
      "outputs": [],
      "source": [
        "z = np.linspace(-6,6,100)\n",
        "plt.plot(z, 1/(1+np.exp(-z)))\n",
        "plt.title('The logistic sigmoid function')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz9Fsv1m4uHA"
      },
      "source": [
        "### Properties of the logistic sigmoid\n",
        "\n",
        "The logistic sigmoif has a few nice properties:\n",
        "\n",
        "1. $\\sigma(0) = 0.5$. This means that points exaclt on the separating line are ambigous, just like we desired.\n",
        "2. $\\lim_{z\\rightarrow \\infty} \\sigma(z) = 1$ and $\\lim_{z\\rightarrow -\\infty} \\sigma(z) = 0$: the further we are from the separating boundary, the more confident the model.\n",
        "3. $\\sigma(-z) = 1 - \\sigma(z)$. The function is symmetrical, it doesn't matter which class we treat as the \"positive\" one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZUJAt7M4uHB"
      },
      "source": [
        "### Definition of the Logistic Regression model\n",
        "\n",
        "Armed with the $\\sigma$ squashing function, let's define:\n",
        "\n",
        "$$\n",
        "p(y=1|x;\\Theta) = \\sigma(x\\Theta),\n",
        "$$\n",
        "where $x$ is a row vector of features $y$ is a random variable denoting the target class and $\\Theta$ is a column vector of model parameters.\n",
        "\n",
        "**remark** In the scatter plot above the guesstimated line is represented by $\\Theta$ equal to `[-6.5, 1., 1.]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idj9b0Ie4uHB"
      },
      "source": [
        "Please observe that:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p(y=y^{(i)}|x^{(i)};\\Theta) &= \\cases{p(y=1|x^{(i)};\\Theta) &if $y^{(i)}=1$ \\\\ 1-p(y=1|x^{(i)};\\Theta) &if $y^{(i)}=0$} \n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWSow61T4uHB"
      },
      "source": [
        "Therefore the cost function of the fitness of the model which we can use is:\n",
        "$$\n",
        "\\begin{split}\n",
        "J(\\Theta) &= -\\sum_{i=1}^{N} y^{(i)} \\log \\sigma(x^{(i)}\\Theta) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)}\\Theta)) = \\\\\n",
        "&= -\\sum_{i=1}^{N}y^{(i)}\\log p(y=1|x^{(i)}; \\Theta) + (1-y^{(i)})\\log  p(y=0|x^{(i)}; \\Theta)\n",
        "\\end{split}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZUV--q54uHC"
      },
      "source": [
        "## Logistic regression training\n",
        "\n",
        "We now need to find the $\\Theta$ which minimizes the $J(\\Theta)$ function. Again, we will use calculus and compute the gradient of the nll. We will do it in steps. First please observe how simple is the gradient of the logistic sigmoid function:\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial z}\\sigma(z) = \\sigma(z)(1-\\sigma(z)) $$\n",
        "\n",
        "Now let $z^{(i)} = x^{(i)}\\Theta$. First let compute the gradient of the loss on a single sample $J^{(i)} = y^{(i)} \\log \\sigma(x^{(i)}\\Theta) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)}\\Theta))$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J^{(i)}(\\Theta)}{\\partial \\Theta} = \\frac{\\partial J^{(i)}}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial\\Theta}\n",
        "$$\n",
        "\n",
        "The derivative of the first term on the right hand turns out to be very simple:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J^{(i)}}{\\partial z^{(i)}} &= \\frac{\\partial -\\left(y^{(i)} \\log \\sigma(z^{(i)}) + (1-y^{(i)})\\log(1-\\sigma(z^{(i)}))\\right)}{\\partial z^{(i)}} \\\\\n",
        "&= -y^{(i)}\\frac{\\sigma(z^{(i)})(1-\\sigma(z^{(i)}))}{\\sigma(z^{(i)})} + \n",
        "(1-y^{(i)})\\frac{\\sigma(z^{(i)})(1-\\sigma(z^{(i)}))}{1-\\sigma(z^{(i)})} \\\\\n",
        "&= -y^{(i)}(1-\\sigma(z^{(i)})) + (1-y^{(i)})\\sigma(z^{(i)})\\\\\n",
        "&=\\sigma(z^{(i)})-y^{(i)}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The second term $\\frac{\\partial z^{(i)}}{\\partial\\Theta}$ is also easy:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z^{(i)}}{\\partial\\Theta} = \\frac{\\partial x^{(i)}\\Theta}{\\partial\\Theta} = (x^{(i)})^T.\n",
        "$$\n",
        "\n",
        "Taken together the loss derivative is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\Theta)}{\\partial \\Theta} = \\sum_{i=1}^{N}\\frac{\\partial J^{(i)}}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial\\Theta} = \\sum_{i=1}^{N}(x^{(i)})^T\\left(\\sigma(z^{(i)})-y^{(i)}\\right)\n",
        "$$\n",
        "\n",
        "The expression can be further simplified by using the data matrix $X$ (of shape $N\\times D$ whose $i$-th row is the $i$-th sample), the target matrix $Y$, letting $Z=X\\Theta$, and assuming that the function $\\sigma()$ is applied separately to all elements of its input.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\Theta)}{\\partial \\Theta} = X^T\\left(\\sigma(Z)-Y\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMtPNeI34uHC"
      },
      "source": [
        "### The Gradient Descent algorithm\n",
        "\n",
        "Due to the nonlinear $\\sigma$ function appearing in the gradient of neg-log likelihood for logistic regression, we can't derive a closed form formula for the zero-crossing of the gradient.\n",
        "\n",
        "Instead we will follow an iterative minimization procedure. Recall that the negated gradient of a function points in the direction of the maximal function decrease (in fact, one can define the gradient to be direction of maximum function increase for an inifitesimally small step).\n",
        "\n",
        "Our approach to solving the linear regression problem will follow this intuition: we will evaluate the neg-log likelihood, take its gradient, then make a small step against the gradient, hopefully getting a smaller neg-log likelihood value. After many such steps we hope to get to a local minimum - a point where the gradient is zero, and we can't go any lower.\n",
        "\n",
        "It turns out, that again our optimization problem is convex, and the local minimum reached with gradient descent will be the global one.\n",
        "\n",
        "The gradient descent algorithm is very simple:\n",
        "\n",
        "* $\\Theta \\gets $ a sane initial value\n",
        "* `While` not converged:\n",
        "    * \n",
        "$\\Theta \\gets \\Theta - \\alpha \\frac{\\partial J(\\Theta)}{\\partial\\Theta}$\n",
        "\n",
        "When GD converges, gradient is close to zero and neg-log likelihood stops to change. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C06nYED4uHC"
      },
      "source": [
        "### How does it look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdd-R4XO4uHC"
      },
      "outputs": [],
      "source": [
        "#give a method for computing the gradient\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def loss(Theta, X, y):\n",
        "    p = sigmoid(X @ Theta )\n",
        "    nll = -(y.T @ np.log(p) + (1 - y.T) @ np.log(1.0 - p))\n",
        "    return nll[0]\n",
        "\n",
        "def grad(Theta, X, y):\n",
        "    p = sigmoid(X @ Theta )\n",
        "    grad = -X.T @ (y - p)\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZPVz8Qg4uHD",
        "outputId": "fa529f60-a42b-4437-e8ad-035d854bf0de",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "Theta = dict()\n",
        "Theta[0] = np.zeros((3, 1)) #sane starting Theta\n",
        "\n",
        "alpha=1e-2\n",
        "for t in range(100000):\n",
        "  #the essential part of the loop\n",
        "  Theta[t+1] = Theta[t] - alpha*grad(Theta[t], IrisX, IrisY)\n",
        "  \n",
        "  #print stuff from time to time\n",
        "  if t % 1000 == 0:\n",
        "    print(\"Iter\", t,\"\\tTheta:\", Theta[t].flatten(), \"loss:\", loss(Theta[t], IrisX, IrisY))\n",
        "  \n",
        "  #convergence criterion -- if new Theta doesn't change a lot, then stop\n",
        "  if np.linalg.norm(Theta[t+1] - Theta[t])  < 1e-4:\n",
        "    break\n",
        "\n",
        "ThetaOpt = Theta[t+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqx8u6kq4uHD"
      },
      "source": [
        "And that's it!\n",
        "Now let's look at the visualization of the process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSCYOspx4uHD"
      },
      "outputs": [],
      "source": [
        "Theta_hist = [Theta[it] for it in range(t)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "BwrLAA-E4uHD",
        "outputId": "cc55177c-7c7a-4305-a92e-db0ca1a456dd"
      },
      "outputs": [],
      "source": [
        "# Make an animation of training\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY, cmap=\"spring\")\n",
        "plt.xlabel(\"petal_length\")\n",
        "plt.ylabel(\"petal_width\")\n",
        "\n",
        "xlim = np.array(plt.xlim())\n",
        "ylim = plt.ylim()\n",
        "\n",
        "line, = plt.plot([],[])\n",
        "\n",
        "plt.close()\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    return line,\n",
        "\n",
        "def animate(i):\n",
        "    epoch = 1 + int(1.0 * (len(Theta_hist)-1) / num_frames * i)\n",
        "    Theta = Theta_hist[epoch]\n",
        "    ax.set_title('Epoch %d, Theta: %s' % (epoch, Theta,))\n",
        "    yy = (Theta[0] + xlim * Theta[1]) / - Theta[2]\n",
        "    line.set_data(xlim, yy)\n",
        "    return line,\n",
        "\n",
        "num_frames=100\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                               frames=100, interval=30, blit=True)\n",
        "rc('animation', html='jshtml')\n",
        "anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5NxkUiG4uHE"
      },
      "source": [
        "## Synthetic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaVn17kE4uHE"
      },
      "source": [
        "Now let's work on synthetic data.\n",
        "\n",
        "1) first fix $X$ data (in our case random but we don't need it to be random);\n",
        "\n",
        "2) then we fix the separating line. In our case we are going to assume the separating line corresponds to an equation $x_1 - x_2 = 0$ (note that we don't have any free term corresponding to shifting);\n",
        "\n",
        "3) then we generate **randomly** $y$ labels according to the $\\sigma(x \\cdot \\Theta)$. We are going to do it assuming that the probability of a label being 1 is $p(y=1|x;\\Theta) = \\sigma(x\\Theta)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZVxiSFA4uHE"
      },
      "outputs": [],
      "source": [
        "Theta_true = [1, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKt6nWwD4uHE"
      },
      "outputs": [],
      "source": [
        "n = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_-KXYAr4uHE",
        "outputId": "64ea1e96-0f34-47fd-a73a-b5da43b41683"
      },
      "outputs": [],
      "source": [
        "X = np.random.normal(size=(n,2))\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwvOJxNzigy9"
      },
      "outputs": [],
      "source": [
        "y_proba = sigmoid(X @ Theta_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVmyOz8k4uHE"
      },
      "outputs": [],
      "source": [
        "y = (np.random.uniform(size=(n)) > y_proba).astype(int).reshape(n,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2z9x53L4uHF",
        "outputId": "e6d51f64-6a94-4c8b-951d-35cafc53025b"
      },
      "outputs": [],
      "source": [
        "loss(Theta_true, X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "b3GtZmZt4uHF",
        "outputId": "f1101346-61ea-4c5a-fce5-1659edbf7698"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"cool\")\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "ylim = plt.ylim()\n",
        "print(ylim)\n",
        "\n",
        "# Guesstimate a separation boundary\n",
        "plt.plot(plt.xlim(), np.array(plt.xlim()))\n",
        "plt.ylim(*ylim)\n",
        "_ = plt.text(3,2,\"x1 - x2 = 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0FZUVEz4uHG"
      },
      "outputs": [],
      "source": [
        "class GradientDescentLogisticRegression:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y, verbose=True):\n",
        "        Theta = dict()\n",
        "        Theta[0] = np.array([[4.],\n",
        "                             [4.]])#np.zeros((X.shape[1], 1)) #sane starting Theta\n",
        "\n",
        "        alpha=1e-3\n",
        "        for t in range(1000):\n",
        "          #the essential part of the loop\n",
        "          Theta[t+1] = Theta[t] - alpha*grad(Theta[t], X, y)\n",
        "\n",
        "          #print stuff from time to time\n",
        "          if (verbose) and (t % 100 == 0):\n",
        "            print(\"Iter\", t,\"\\t\\tTheta:\", Theta[t].flatten(), \"loss:\", loss(Theta[t], X, y))\n",
        "\n",
        "          #convergence criterion\n",
        "          if np.linalg.norm(Theta[t+1] - Theta[t])  < 1e-4:\n",
        "            break\n",
        "            \n",
        "        if(verbose):\n",
        "            print(\"Iter\", t,\"\\t\\tTheta:\", Theta[t].flatten(), \"loss:\", loss(Theta[t], X, y))\n",
        "        self.ThetaOpt = Theta[t+1]\n",
        "        self.Theta_hist = np.asarray([Theta[it] for it in range(t)])\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return np.round(sigmoid(X @ self.ThetaOpt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAm-LIs24uHG"
      },
      "outputs": [],
      "source": [
        "lr_model = GradientDescentLogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAgcl3r4uHH",
        "outputId": "41bed759-1418-4c30-bf2d-0594d15de823"
      },
      "outputs": [],
      "source": [
        "lr_model.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JIEhdsT4uHH",
        "outputId": "f41c50d0-414c-4f1e-a692-c090704193e6"
      },
      "outputs": [],
      "source": [
        "lr_model.ThetaOpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "Ti2AwRPp4uHH",
        "outputId": "719a73ed-2db1-4cfe-c55b-6570876c4eff"
      },
      "outputs": [],
      "source": [
        "# Make an animation of training\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"cool\")\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "# Guesstimate a separation boundary\n",
        "plt.plot(plt.xlim(), np.array(plt.xlim()))\n",
        "plt.ylim(*ylim)\n",
        "_ = plt.text(3,2,\"x1 - x2 = 0\")\n",
        "\n",
        "xlim = np.array(plt.xlim())\n",
        "ylim = plt.ylim()\n",
        "\n",
        "line, = plt.plot([],[])\n",
        "\n",
        "plt.close()\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    return line,\n",
        "\n",
        "def animate(i):\n",
        "    epoch = 1 + int(1.0 * (len(lr_model.Theta_hist)-1) / num_frames * i)\n",
        "    Theta = lr_model.Theta_hist[epoch]\n",
        "    ax.set_title('Epoch %d, Theta: %s' % (epoch, Theta,))\n",
        "    yy = ( xlim * Theta[0]) / - Theta[1]\n",
        "    line.set_data(xlim, yy)\n",
        "    return line,\n",
        "\n",
        "num_frames=100\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                               frames=100, interval=30, blit=True)\n",
        "rc('animation', html='jshtml')\n",
        "anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33MSFYSB4uHI"
      },
      "source": [
        "## visualizing gradient descent in the parameter space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At57BmrW4uHI"
      },
      "outputs": [],
      "source": [
        "delta = 0.025\n",
        "grid_x = np.arange(-5.0, 5.0, delta)\n",
        "grid_y = np.arange(-5.0, 5.0, delta)\n",
        "grid_X, grid_Y = np.meshgrid(grid_x, grid_y)\n",
        "grid_Z = np.zeros(grid_X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCJd5tzE4uHI"
      },
      "outputs": [],
      "source": [
        "for xit in range(len(grid_x)):\n",
        "    for yit in range(len(grid_y)):\n",
        "        grid_Theta = np.array([[grid_x[len(grid_x)-1-xit]],[grid_y[len(grid_y)-1-yit]]])\n",
        "        grid_Z[xit,yit] = loss( grid_Theta, X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "GOh__wjh4uHI",
        "outputId": "046eaef8-4995-4946-c733-632f1521e93c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "CS = ax.contour(grid_x, grid_y, grid_Z, levels=[550, 600,700,900, 1200, 1600, 2000, 2400, 2800])\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "ax.set_title('Simplest default with labels')\n",
        "\n",
        "ax.plot(lr_model.Theta_hist[:, 0], lr_model.Theta_hist[:, 1], color=\"r\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GVxI8lD4uHJ"
      },
      "source": [
        "# Problem [3p]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCaJ5sfP4uHJ"
      },
      "source": [
        "Do the same for the polynomial Ridge regression with degree equal to 44.\n",
        "\n",
        "**Problem 1 [1p]** Calculate the gradients of the loss function $J_{\\mbox{reg}}(\\Theta) = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - x^{(i)}\\Theta)^2 +  \\frac{\\alpha}{N} \\sum_j \\Theta_j^2$, i.e., provide the calculations in the markdown below. You can either do it in LaTeX or you can insert a reasonably good looking photo of your calculations on a sheet of paper.\n",
        "\n",
        "**Problem 2 [2p]** Implement the gradient descent that calculates $\\Theta_{\\text{OPT}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eksom9aK4uHJ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# The true polynomial relation:\n",
        "# y(x) = 1 + 2x -5x^2 + 4x^3\n",
        "#\n",
        "\n",
        "true_poly_theta = np.array(\n",
        "    [1.0, 2.0, -5, 4]\n",
        ")\n",
        "\n",
        "def powers_of_X(X, degree):\n",
        "    powers = np.arange(degree + 1).reshape(1, -1)\n",
        "    return X ** powers\n",
        "\n",
        "\n",
        "def compute_polynomial(X, Theta):\n",
        "    XP = powers_of_X(X, len(Theta) - 1)  \n",
        "    Y = XP @ Theta.reshape(-1, 1)  \n",
        "    return Y\n",
        "\n",
        "def make_dataset(N, theta=true_poly_theta, sigma=0.1):\n",
        "    \"\"\" Sample a dataset \"\"\"\n",
        "    X = np.random.uniform(size=(N, 1))\n",
        "    Y_clean = compute_polynomial(X, theta)\n",
        "    Y = Y_clean + np.random.randn(N, 1) * sigma\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "train_data = make_dataset(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "o0pQzVBQ4uHK",
        "outputId": "fea2a558-d3e6-4835-8bf7-e66e5e95a4d5"
      },
      "outputs": [],
      "source": [
        "XX = np.linspace(train_data[0].min(), train_data[0].max(), 100).reshape(-1, 1)\n",
        "YY = compute_polynomial(XX, true_poly_theta)\n",
        "plt.scatter(train_data[0], train_data[1], label=\"train data\", color=\"r\")\n",
        "plt.plot(XX, compute_polynomial(XX, true_poly_theta), label=\"ground truth\")\n",
        "plt.legend(loc=\"upper left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVLw9JUM4uHK"
      },
      "outputs": [],
      "source": [
        "X, y = train_data[0], train_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNHgHWCu4uHK"
      },
      "outputs": [],
      "source": [
        "# def grad(Theta, X, y):\n",
        "#     p = sigmoid(X @ Theta )\n",
        "#     grad = -X.T @ (y - p)\n",
        "#     return grad\n",
        "\n",
        "class GradientDescentPolyRidgeRegression:\n",
        "    def __init__(self, deg, alpha):\n",
        "        self.deg = deg\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def loss(self, X, y, Theta):\n",
        "        m = np.size(X, axis = 0)\n",
        "        predictions = X*Theta\n",
        "        sqrErrors = np.multiply((predictions - y),(predictions - y))\n",
        "        j = 1/(2*m)*np.sum(sqrErrors)\n",
        "        return j\n",
        "    \n",
        "    def grad( self, X, Theta, y):\n",
        "        return (2.0/X.shape[0] )* (np.dot(X.T, (X.dot(Theta) - y)) + self.alpha * Theta)\n",
        "    \n",
        "    def fit(self, X, y, verbose=True):\n",
        "        nArr2D = np.tile(X, self.deg +1) \n",
        "        for index in range(self.deg +1):\n",
        "          column = nArr2D[:, index]\n",
        "          nArr2D[:, index] = np.power(column, index)\n",
        "        Theta = np.zeros((nArr2D.shape[1], 1))\n",
        "        lr = 0.01\n",
        "        iteration_number = 0\n",
        "        for i in range(10000):\n",
        "            Theta_new = Theta - lr * self.grad(nArr2D, Theta, y)\n",
        "            if np.linalg.norm(Theta_new - Theta)  < 1e-4:\n",
        "               break\n",
        "            Theta = Theta_new\n",
        "            iteration_number +=1\n",
        "        self.Theta = Theta\n",
        "        return Theta\n",
        "        \n",
        "    def predict(self, X):\n",
        "        nArr2D = np.tile(X, self.deg +1) \n",
        "        for index in range(self.deg +1):\n",
        "          column = nArr2D[:, index]\n",
        "          nArr2D[:, index] = np.power(column, index)\n",
        "        return nArr2D.dot(self.Theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cMxwFlj4uHK"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return ((y_pred - y_true)**2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "crgsub5P4uHK",
        "outputId": "11caac3a-0f42-4d83-8a69-b1c9acce4bb9"
      },
      "outputs": [],
      "source": [
        "polyridge_model = GradientDescentPolyRidgeRegression(deg=30, alpha=0)\n",
        "polyridge_model.fit(X,y, verbose=False)\n",
        "y_pred = polyridge_model.predict(X)\n",
        "print(\"MSE:\",mse(y, y_pred))\n",
        "\n",
        "XX = np.linspace(train_data[0].min(), train_data[0].max(), 100).reshape(-1, 1)\n",
        "plt.scatter(train_data[0], train_data[1], label=\"train data\", color=\"r\")\n",
        "plt.plot(XX, compute_polynomial(XX, true_poly_theta), label=\"ground truth\")\n",
        "plt.plot(XX, polyridge_model.predict(XX), label=\"predicted\")\n",
        "plt.legend(loc=\"upper left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "HbZl8D3C4uHL",
        "outputId": "da539302-5b96-44b4-a4d1-36087e31f90a"
      },
      "outputs": [],
      "source": [
        "polyridge_model = GradientDescentPolyRidgeRegression(deg=10, alpha=1.44)\n",
        "polyridge_model.fit(X,y, verbose=False)\n",
        "y_pred = polyridge_model.predict(X)\n",
        "print(\"MSE:\",mse(y, y_pred))\n",
        "\n",
        "XX = np.linspace(train_data[0].min(), train_data[0].max(), 100).reshape(-1, 1)\n",
        "plt.scatter(train_data[0], train_data[1], label=\"train data\", color=\"r\")\n",
        "plt.plot(XX, compute_polynomial(XX, true_poly_theta), label=\"ground truth\")\n",
        "plt.plot(XX, polyridge_model.predict(XX), label=\"predicted\")\n",
        "plt.legend(loc=\"upper left\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, May 18 2022, 14:25:27) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "vscode": {
      "interpreter": {
        "hash": "0e1e813d9e0606be4c0cf7838e5d609c13b18076b542c53a59882829292efcb6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
